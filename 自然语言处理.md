# 标题：自然语言处理NLP

## 一.入门

### 1.1流程

本文链接：https://blog.csdn.net/u012736685/article/details/88246090

第一步：获取语料
语料，即语言材料，是构成语料库的基本单元。 所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：

1、已有语料
纸质或者电子文本资料==》电子化==》语料库。

2、网上下载、抓取语料
国内外标准开放数据集（比如国内的中文汉语有搜狗语料、人民日报语料） 或 通过爬虫。

第二步：语料预处理
语料预处理大概会占到整个50%-70%的工作量。

基本过程： 数据清洗==》分词==》词性标注==》去停词

1、语料清洗
语料清洗：在语料中找到感兴趣的内容，将不感兴趣、视为噪音的内容清洗删除。包括：对于原始文本提取标题、摘要、正文等信息，对于爬虫，去除广告、标签、HTML、JS等代码和注释。

常见数据清洗方式：人工去重、对齐、删除和标注等，或规则提取内容、正则表达式匹配、根据词性和命名实体提取，编写脚本或代码批处理等。

2、分词
分词：将短文本和长文本处理为最小单位粒度是词或词语的过程。

常见方法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，其中每种方法下面对应许多具体的方法。

难点：歧义识别 和 新词识别。 eg：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”==》上下文信息

3、词性标注
词性标注：对每个词或词语打词类标签，是一个经典的序列标注问题。eg：形容词、动词、名词等。有助于在后面的处理中融入更多有用的语言信息。

词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。

常见方法：基于规则和基于统计的方法。

基于统计的方法：基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。
4、去停用词
停用词：对文本特征没有任何贡献的字词，eg：标点符号、语气、人称等。

注意：根据具体场景决定。eg：在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。

三、特征工程
如何把分词之后的字和词语表示成计算机能够计算的类型。
思路：中文分词的字符串 ==》 向量

两种常用表示模型：

词袋模型（BoW）
词向量
1、词袋模型（BoW）
词袋模型（Bag of Word, BOW)：不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。

2、词向量
词向量：将字、词语转换为向量矩阵的计算模型。

常用的词表示方法：

One-Hot：把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。eg: [0 0 0 0 0 0 0 0 1 0 0 0 0 ... 0]
Word2Vec：其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。
Doc2Vec
WordRank
FastText
第四步：特征选择
关键：如何构造好的特征向量？
==》要选择合适的、表达能力强的特征。

常见的特征选择方法：DF、 MI、 IG、 CHI、WLLR、WFO 六种。

第五步：模型训练
1、模型
对于不同的应用需求，我们使用不同的模型

传统的有监督和无监督等机器学习模型： KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；
深度学习模型： CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。
2、注意事项
（1）过拟合
过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。

常见的解决方法有：

增大数据的训练量；
增加正则化项，如 L1 正则和 L2 正则；
特征选取不合理，人工筛选特征和使用特征选择算法；
采用 Dropout 方法等。
（2）欠拟合
欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。

常见的解决方法有：

添加其他特征项；
增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；
减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。
（3）对于神经网络，注意梯度消失和梯度爆炸问题。
————————————————
版权声明：本文为CSDN博主「__盛夏光年__」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u012736685/article/details/88246090





目前的标记集里有26个基本词类标记（名词n、时间词t、处所词s、方位词f、数词m、量词q、区别词b、代词r、动词v、形容词a、状态词z、副词d、介词p、连词c、助词u、语气词y、叹词e、拟声词o、成语i、习惯用语l、简称j、前接成分h、后接成分k、语素g、非语素字x、标点符号w）外，从语料库应用的角度，增加了专有名词（人名nr、地名ns、机构名称nt、其他专有名词nz）；从语言学角度也增加了一些标记，总共使用了40多个个标记。



HanLP         github仓库：https://github.com/hankcs/HanLP 开发语言：Java支持语言：Java，如果使用Python，可以借助JPype
复旦 FundanNLP            github仓库：https://github.com/FudanNLP/fnlp  开发语言：Java支持语言：Java
哈工大 HIT LTP              github地址：https://github.com/HIT-SCIR/ltp开发语言：C++支持语言：Python(pyltp), Java(ltp4j)
 清华 THU LAC                github地址: https://github.com/thunlp/THULAC-Python
5 （中科院）NLPIR (原ICTCLAS) github地址: https://github.com/NLPIR-team/NLPIR
6 斯坦福 Stanford CoreNLP（多语言）github地址: https://nlp.stanford.edu/software/



### 1.2标记说明

| 代码 | 名称     |
| ---- | -------- |
| Ag   | 形语素   |
| a    | 形容词   |
| ad   | 副形词   |
| an   | 名形词   |
| Bg   | 区别语素 |
| b    | 区别词   |
| c    | 连词     |
| Dg   | 副语素   |
| d    | 副词     |
| e    | 叹词     |
| f    | 方位词   |
| g    | 语素     |
| h    | 前接成分 |
| i    | 成语     |
| j    | 简略语   |
| k    | 后接成分 |
| l    | 习用语   |
| Mg   | 数语素   |
| m    | 数词     |
| Ng   | 名语素   |
| n    | 名词     |
| nr   | 人名     |
| ns   | 地名     |
| nt   | 机构团体 |
| nx   | 外文字符 |
| nz   | 其它专名 |
| o    | 拟声词   |
| p    | 介词     |
| Qg   | 量语素   |
| q    | 量词     |
| Rg   | 代语素   |
| r    | 代词     |
| s    | 处所词   |
| Tg   | 时间语素 |
| t    | 时间词   |
| Ug   | 助语素   |
| u    | 助词     |
| Vg   | 动语素   |
| v    | 动词     |
| vd   | 副动词   |
| vn   | 名动词   |
| w    | 标点符号 |
| x    | 非语素字 |
| Yg   | 语气语素 |
| y    | 语气词   |
| z    | 状态词   |

### 1.3中文分词

#### 1.3.1算法

**目前有三大主流分词方法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法。**

##### 1、基于字符串匹配的分词方法

1.1正向最大匹配

1.2逆向最大匹配

1.3双向最大匹配

##### 2、基于理解的分词方法

其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。

##### 3、基于统计的分词方法

主要思想：每个字都是词的最小单元，如果相连的字在不同的文本中出现的频率越多，这就越有可能是一个词。因此我们可以用相邻字出现的频率来衡量组词的可能性，当频率高于某个阈值时，我们可以认为这些字可能会构成一个词。 
<u>主要统计模型： N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model，HMM），最大熵模型（ME），条件随机场（Conditional Random Fields，CRF）等</u> 
优势：在实际运用中常常将字符串匹配分词和统计分词结合使用，这样既体现了匹配分词速度快、效率高的优点，同时又能运用统计分词识别生词、自动消除歧义等方面的特点。

3.1 N-gram模型思想

3.2 隐马尔可夫模型（HMM）

#### 1.3.2中文分词工具介绍

python常用的分词包有jieba分词、SnowNLP、THULAC、NLPIR 等。

##### 1、jieba分词

jieba分词是国内使用人数最多的中文分词工具。

1.1、jieba分词的三种模式
（1）精确模式：试图将句子最精确地切分，适合文本分析； 
（2）全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义； 
（3）搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

jieba分词测试如下

```python
import jieba
u="我来到北京清华大学"
#全模式
test1 = jieba.cut(u, cut_all=True)
print("全模式: " + "| ".join(test1))
#精确模式
test2 = jieba.cut(u, cut_all=False)
print("精确模式: " + "| ".join(test2))
#搜索引擎模式
test3= jieba.cut_for_search(u)  
print("搜索引擎模式:" + "| ".join(test3))
```


全模式: 我| 来到| 北京| 清华| 清华大学| 华大| 大学 
精确模式: 我| 来到| 北京| 清华大学 
搜索引擎模式:我| 来到| 北京| 清华| 华大| 大学| 清华大学

##### 2、SnowNLP

SnowNLP可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。SnowNLP主要包括如下几个功能： 
（1）中文分词（Character-Based Generative Model）； 
（2）词性标注（3-gram HMM）； 
（3）情感分析（简单分析，如评价信息）； 
（4）文本分类（Naive Bayes） 
（5）转换成拼音（Trie树实现的最大匹配） 
（6）繁简转换（Trie树实现的最大匹配） 
（7）文本关键词和文本摘要提取（TextRank算法） 
（8）计算文档词频（TF，Term Frequency）和逆向文档频率（IDF，Inverse Document Frequency） 
（9）Tokenization（分割成句子） 
（10）文本相似度计算（BM25） 
SnowNLP的最大特点是特别容易上手，用其处理中文文本时能够得到不少有意思的结果，但不少功能比较简单，还有待进一步完善。

##### 3、THULAC

THULAC由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点： 
（1）能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 
（2）准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 
（3）速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。

##### 4、NLPIR

NLPIR分词系统是由北京理工大学张华平博士研发的中文分词系统，经过十余年的不断完善，拥有丰富的功能和强大的性能。NLPIR是一整套对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。主要功能包括：中文分词，词性标注，命名实体识别，用户词典、新词发现与关键词提取等功能。











## 二.NLP前端技术解析

### 2.2正则表达式及相关python

**了解**

​	re.search()方法扫描整个字符串，并返回第一个成功的匹配。如果匹配失败，则返回None。

​	re.search()并不要求必须从字符串的开头进行匹配，也就是说，正则表达式可以是字符串的一部分。

```python
re.search(pattern, string, flags
```

pattern : 正则中的模式字符串。

string : 要被查找替换的原始字符串

## 三、名词分析

![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2852955562,3313921744&fm=173&app=49&f=JPEG?w=600&h=359&s=61323C72224277454995A2570300B0E6)

### 1.CRF条件随机场，一种机器学习技术（模型）

使用双向LSTM+CRFs 模型用于NLP序列标注问题（POS、分块、命名实体识别）

CRF分词原理
1. CRF把分词当做字的词位分类问题，通常定义字的词位信息如下：

词首，常用B表示
词中，常用M表示
词尾，常用E表示
单子词，常用S表示

2. CRF分词的过程就是对词位标注后，将B和E之间的字，以及S单字构成分词

3. CRF分词实例：

原始例句：我爱北京天安门
CRF标注后：我/S 爱/S 北/B 京/E 天/B 安/M 门/E

分词结果：我/爱/北京/天安门

### 2.序列标注

序列标注问题是自然语言中最常见的问题，在深度学习火起来之前，常见的序列标注问题的解决方案都是借助于HMM模型，最大熵模型，CRF模型。尤其是CRF，是解决序列标注问题的主流方法。随着深度学习的发展，RNN在序列标注问题中取得了巨大的成果。而且深度学习中的end-to-end，也让序列标注问题变得更简单了。

　　**序列标注问题**包括自然语言处理中的**分词**，**词性标注**，**命名实体识别**，关键词抽取，**词义角色标注**等等。我们只要在做序列标注时给定特定的标签集合，就可以进行序列标注。

　　序列标注问题是NLP中最常见的问题，因为绝大多数NLP问题都可以转化为序列标注问题，虽然很多NLP任务看上去大不相同，但是如果转化为序列标注问题后其实面临的都是同一个问题。所谓“序列标注”，就是说对于一个一维线性输入序列：

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725205710497-99721212.png)

　　给线性序列中的每个元素打上标签集合中的某个标签：

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725205744384-394040534.png)

　　所以，其本质上是对线性序列中每个元素根据上下文内容进行分类的问题。一般情况下，对于NLP任务来说，线性序列就是输入的文本，往往可以把一个汉字看做线性序列的一个元素，而不同任务其标签集合代表的含义可能不太相同，但是相同的问题都是：如何根据汉字的上下文给汉字打上一个合适的标签（无论是分词，还是词性标注，或者是命名实体识别，道理都是想通的）。

**序列标注问题之中文分词**

　　以中文分词任务来说明序列标注的过程。假设现在输入句子“跟着TFboys学左手右手一个慢动作”，我们的任务是正确地把这个句子进行分词。首先，把句子看做是一系列单字组成的线性输入序列，即：

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210026436-92172388.png)

　　序列标注的任务就是给每个汉字打上一个标签，对于分词任务来说，我们可以定义标签集合为（jieba分词中的标签集合也是这样的）：

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210047308-899941442.png)

　　其中B代表这个汉字是词汇的开始字符，M代表这个汉字是词汇的中间字符，E代表这个汉字是词汇的结束字符，而S代表单字词。

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210132453-1779204887.png)

　　有了这四个标签就可以对中文进行分词了。这时你看到了，中文分词转换为对汉字的序列标注问题，假设我们已经训练好了序列标注模型，那么分别给每个汉字打上标签集合中的某个标签，这就算是分词结束了，因为这种形式不方便人来查看，所以可以增加一个后处理步骤，把B开头，后面跟着M的汉字拼接在一起，直到碰见E标签为止，这样就等于分出了一个单词，而打上S标签的汉字就可以看做是一个单字词。于是我们的例子就通过序列标注，被分词成如下形式：

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210218111-1941846063.png)

　　在这里我们可以采用双向LSTM来处理该类问题，双向会关注上下文的信息。

　　在NLP中最直观的处理问题的方式就是要把问题转换为序列标注问题，思考问题的思维方式也就转换为序列标注思维，这个思维很重要，决定你能否真的处理好NLP问题。

**序列标注之命名实体识别（NER）**

　　我们再来看看命名实体识别问题中的序列标注，命名实体识别任务是识别句子中出现的实体，通常识别人名、地名、机构名这三类实体。现在的问题是：假设输入中文句子

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210731375-1237953271.png)

　　我们要识别出里面包含的人名、地名和机构名。如果以序列标注的角度看这个问题，我们首先得把输入序列看成一个个汉字组成的线性序列，然后我们要定义标签集合，标签集合如下（在这里的标签用什么代表不重要，重要的是它代表的含义）：　　

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210924636-717053426.png)

　　其中，BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。

　　　　![img](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180725210947431-509998129.png)

　　有了输入汉字序列，也有了标签集合，那么剩下的问题是训练出一个序列标注ML系统，能够对每一个汉字进行分类，假设我们已经学好了这个系统，那么就给输入句子中每个汉字打上标签集合中的标签，于是命名实体就被识别出来了，为了便于人查看，增加一个后处理步骤，把人名、地名、机构名都明确标识出来即可。

　　除了上面的分词和命名实体标注，很多其他的NLP问题同样可以转换为序列标注问题，比如词性标注、CHUNK识别、句法分析、语义角色识别、关键词抽取等。

　　传统解决序列标注问题的方法包括HMM/MaxEnt/CRF等，很明显RNN很快会取代CRF的主流地位，成为解决序列标注问题的标准解决方案，那么如果使用RNN来解决各种NLP基础及应用问题，我们又该如何处理呢，下面我们就归纳一下使用RNN解决序列标注问题的一般优化思路。

　　对于分词、词性标注（POS）、命名实体识别（NER）这种前后依赖不会太远的问题，可以用RNN或者BiRNN处理就可以了。而对于具有长依赖的问题，可以使用LSTM、RLSTM、GRU等来处理。关于GRU和LSTM两者的性能差不多，不过对于样本数量较少时，有限考虑使用GRU（模型结构较LSTM更简单）。此外神经网络在训练的过程中容易过拟合，可以在训练过程中加入Dropout或者L1/L2正则来避免过拟合。

**CRF和LSTM在序列标注上的优劣**

　　**LSTM：**像RNN、LSTM、BILSTM这些模型，它们在序列建模上很强大，它们能够capture长远的上下文信息，此外还具备神经网络拟合非线性的能力，这些都是crf无法超越的地方，对于t时刻来说，输出层yt受到隐层ht（包含上下文信息）和输入层xt（当前的输入）的影响，但是yt和其他时刻的yt`是相互独立的，感觉像是一种point wise，对当前t时刻来说，我们希望找到一个概率最大的yt，但其他时刻的yt`对当前yt没有影响，如果yt之间存在较强的依赖关系的话（例如，形容词后面一般接名词，存在一定的约束），LSTM无法对这些约束进行建模，LSTM模型的性能将受到限制。

　　**CRF：**它不像LSTM等模型，能够考虑长远的上下文信息，它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的yt，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种联合概率，优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。

　　**HMM：**CRF不管是在实践还是理论上都要优于HMM，HMM模型的参数主要是“初始的状态分布”，“状态之间的概率转移矩阵”，“状态到观测的概率转移矩阵”，这些信息在CRF中都可以有，例如：在特征模版中考虑h(y1), f(yi-1, yi), g(yi, xi)等特征。

　　**CRF与LSTM：**从数据规模来说，在数据规模较小时，CRF的试验效果要略优于BILSTM，当数据规模较大时，BILSTM的效果应该会超过CRF。从场景来说，如果需要识别的任务不需要太依赖长久的信息，此时RNN等模型只会增加额外的复杂度，此时可以考虑类似科大讯飞**FSMN**（一种基于窗口考虑上下文信息的“前馈”网络）。

　　**CNN＋BILSTM＋CRF：**这是目前学术界比较流行的做法，BILSTM＋CRF是为了结合以上两个模型的优点，CNN主要是处理英文的情况，英文单词是由更细粒度的字母组成，这些字母潜藏着一些特征（例如：前缀后缀特征），通过CNN的卷积操作提取这些特征，在中文中可能并不适用（中文单字无法分解，除非是基于分词后），这里简单举一个例子，例如词性标注场景，单词football与basketball被标为名词的概率较高， 这里后缀ball就是类似这种特征。





# 标题：自然语言处理之flask

## 一.路由信息

**路由基本定义**

- 设置路由,路径,参数,请求方式
- PostMan 的使用

### 1.指定路由地址

```python
# 指定访问视图函数demo1,访问路径为/demo1
@app.route('/demo1')
def demo1():
    return 'demo1'
```

### 2.给路由传参示例

有时我们需要将同一类 URL 映射到同一个视图函数处理，比如：使用同一个视图函数来显示不同用户的个人信息。

```python
# 路由传递参数,整数
@app.route('/user/<int:user_id>')
def user_info(user_id):
    return 'the num is %d' % user_id
# 路由传递参数,字符串,不指定path默认就是字符串
@app.route('/user/<path:user_id>')
def user_info(user_id):
    return 'hello %s' % user_id
```

> 提示:之所以int,path可以接收整数,字符串,是由于werkzeug提供了IntegerConverter,PathConverter对应转换器.

### 3.指定请求方式

在 Flask 中，定义一个路由，默认的请求方式为：

- GET
- OPTIONS(自带)
- HEAD(自带)

如果想添加请求方试，那么可以使用methods指定,比如：

```python
@app.route('/demo2', methods=['GET', 'POST'])
def demo2():
    # 直接从请求中取到请求方式并返回
    return request.method
```

二

